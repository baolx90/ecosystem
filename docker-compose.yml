version: "3.7"
services:
  hivemetastore:
    image: postgres:11.5
    hostname: hivemetastore
    environment:
      POSTGRES_PASSWORD: new_password
    ports:
      - 5432:5432
    volumes:
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
  zoo1:
    image: zookeeper
    restart: always
    hostname: zoo1
    ports:
      - 2181:2181
    environment:
      ZOO_MY_ID: 1
      ZOO_SERVERS: server.1=zoo1:2888:3888;2181 server.2=zoo2:2888:3888;2181 server.3=zoo3:2888:3888;2181
  zoo2:
    image: zookeeper
    restart: always
    hostname: zoo2
    ports:
      - 2182:2181
    environment:
      ZOO_MY_ID: 2
      ZOO_SERVERS: server.1=zoo1:2888:3888;2181 server.2=zoo2:2888:3888;2181 server.3=zoo3:2888:3888;2181
  zoo3:
    image: zookeeper
    restart: always
    hostname: zoo3
    ports:
      - 2183:2181
    environment:
      ZOO_MY_ID: 3
      ZOO_SERVERS: server.1=zoo1:2888:3888;2181 server.2=zoo2:2888:3888;2181 server.3=zoo3:2888:3888;2181

  master:
    build: .
    hostname: master
    depends_on:
      - hivemetastore
      - zoo1
      - zoo2
      - zoo3
    environment:
      HADOOP_NODE: namenode
      HIVE_CONFIGURE: yes, please
      HBASE_ROLE: hmaster thrift
      HBASE_MASTER_HOSTNAME: master
      SPARK_PUBLIC_DNS: localhost
      SPARK_LOCAL_IP: 127.0.0.1
      SPARK_MASTER_HOST: 127.0.0.1
    expose:
      - 1-65535
    ports:
      # Spark Master Web UI
      - 8080:8080
      # Spark History server
      - 18080:18080
      # YARN UI
      - 8088:8088
      # Hadoop namenode UI
      - 9870:9870
      # Hive JDBC
      - 10000:10000
      # HBase UI
      - 16010:16010
    volumes:
      - ./data:/data
      - ./spark/apps:/usr/spark/apps

  worker:
    build: .
    depends_on:
      - master
    environment:
      SPARK_MASTER_ADDRESS: spark://master:7077
      HADOOP_NODE: datanode
      HBASE_ROLE: regionserver
      HBASE_MASTER_HOSTNAME: master
      SPARK_PUBLIC_DNS: localhost
      SPARK_MASTER_HOST: localhost
    volumes:
      - ./data:/data
      - ./spark/apps:/usr/spark/apps

  kafka:
    image: docker.io/bitnami/kafka:3.5
    hostname: baolx.local
    restart: unless-stopped
    ports:
      - '9092:9092'
      - '9093:9093'
    environment:
      KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE: "true"
      ALLOW_PLAINTEXT_LISTENER: "yes"
      KAFKA_HEAP_OPTS: "-Xms4G -Xmx8G"
      # KRaft settings
      KAFKA_CFG_NODE_ID: 0
      KAFKA_CFG_PROCESS_ROLES: "controller,broker"
      KAFKA_CFG_CONTROLLER_QUORUM_VOTERS: "0@baolx.local:9093"
      KAFKA_KRAFT_CLUSTER_ID: "ZWU5YmZhYjBhNGE5NDA2NW"
      KAFKA_CFG_BROKER_RACK: 1
      KAFKA_CFG_LOG_RETENTION_HOURS: 2
      KAFKA_CFG_NUM_PARTITIONS: 1
      # Listeners
      KAFKA_CFG_LISTENERS: "PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093"
      KAFKA_CFG_ADVERTISED_LISTENERS: "PLAINTEXT://baolx.local:9092"
      KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP: "PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT"
      KAFKA_CFG_CONTROLLER_LISTENER_NAMES: "CONTROLLER"
      KAFKA_CFG_INTER_BROKER_LISTENER_NAME: "PLAINTEXT"

  akhq:
    image: tchiotludo/akhq
    restart: unless-stopped
    depends_on:
      - kafka
    ports:
      - 8081:8080
    environment:
      AKHQ_CONFIGURATION: |
        akhq:
          connections:
            docker-kafka-server:
              properties:
                bootstrap.servers: "baolx.local:9092"
      
  debezium:
    image: debezium/connect:2.4
    container_name: debezium
    restart: unless-stopped
    depends_on:
      - kafka
    ports:
      - 8083:8083
    environment:
      BOOTSTRAP_SERVERS: kafka:9092
      GROUP_ID: 1
      CONFIG_STORAGE_TOPIC: bao_connect_configs
      OFFSET_STORAGE_TOPIC: bao_connect_offsets
      STATUS_STORAGE_TOPIC: bao_connect_statuses
      HEAP_OPTS: -Xms4G -Xmx8G
      CONNECT_TOPIC_CREATION_ENABLE: 'true'
      CONNECT_CONFIG_PROVIDERS: file
      CONNECT_CONFIG_PROVIDERS_FILE_CLASS: org.apache.kafka.common.config.provider.FileConfigProvider
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_PRODUCER_MAX_REQUEST_SIZE: 5048576
      CONNECT_CONSUMER_MAX_PARTITION_FETCH_BYTES: 5048576
      CONNECT_MAX_REQUEST_SIZE: 10485760
      OFFSET_FLUSH_TIMEOUT_MS: 60000  # default 5000
      OFFSET_FLUSH_INTERVAL_MS: 15000  # default 60000
      MAX_BATCH_SIZE: 32768  # default 2048
      MAX_QUEUE_SIZE: 131072  # default 8192
      MAX_REQUEST_SIZE: 10485760  # default 8192
      FETCH_MAX_BYTES: 5048576
      MAX_PARTITION_FETCH_BYTES: 5048576